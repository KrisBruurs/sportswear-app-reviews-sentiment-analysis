ungroup() %>%
pivot_wider( # you will normally Google this
names_from = sentiment,
values_from = n,
values_fill = 0
) %>%
mutate(
sentiment_bing = case_when(
positive > negative ~ "positive",
negative > positive ~ "negative",
.default = "neutral"
)
)
# Compare with sentiment star
comparison_df <- reviews %>%
select(id, sentiment_star) %>%
left_join(sentiment_bing, by = "id") %>%
select(review_id, sentiment_star, sentiment_bing) %>%
mutate(
sentiment_star = as.factor(sentiment_star),
sentiment_bing = as.factor(sentiment_bing)
)
###---Libraries---###
library(tidyverse)
library(tibble)
library(stringr)
library(tidytext)
library(vader)
library(tokenizers)
library(textdata)
library(textstem)
library(yardstick)
###---Input---###
tokens <- read_csv('data/tokenized_reviews.csv')
reviews <- read_csv('data/cleaned_reviews.csv')
###---Process---###
# Sentiment Bing
tokens_bing <- tokens %>%
left_join(get_sentiments('bing')) %>%
mutate(replace_na(sentiment, 'neutral'))
sentiment_bing <- tokens_bing %>%
group_by(id) %>%
count(sentiment) %>%
ungroup() %>%
pivot_wider( # you will normally Google this
names_from = sentiment,
values_from = n,
values_fill = 0
) %>%
mutate(
sentiment_bing = case_when(
positive > negative ~ "positive",
negative > positive ~ "negative",
.default = "neutral"
)
)
# Compare with sentiment star
comparison_df <- reviews %>%
select(id, sentiment_star) %>%
left_join(sentiment_bing, by = "id") %>%
select(id, sentiment_star, sentiment_bing) %>%
mutate(
sentiment_star = as.factor(sentiment_star),
sentiment_bing = as.factor(sentiment_bing)
)
accuracy(comparison_df,
sentiment_star,
sentiment_bing)
conf_mat(comparison_df,
sentiment_star,
sentiment_bing)
# Sentiment Afinn
tokens_afinn <- tokens %>%
left_join(get_sentiments('afinn')) %>%
mutate(value = replace_na(value, 0))
sentiment_afinn <- tokens_afinn %>%
group_by(id) %>%
summarise(score = sum(value, na.rm = TRUE)) %>%
ungroup %>%
mutate(sentiment_afinn = case_when(
score > 0 ~ 'positive',
score < 0 ~ 'negative',
.default = 'neutral'
)) %>%
select(!score)
comparison_df <- comparison_df %>%
left_join(sentiment_afinn, by = 'id')
comparison_df <- comparison_df %>%
mutate(sentiment_afinn = as.factor(sentiment_afinn))
accuracy(comparison_df,
sentiment_star,
sentiment_afinn)
conf_mat(comparison_df,
sentiment_star,
sentiment_afinn)
# compare two methods to themselves
accuracy(comparison_df,
sentiment_bing,
sentiment_afinn)
conf_mat(comparison_df,
sentiment_bing,
sentiment_afinn)
# Sentiment VADER
vader_sent <- vader_df(reviews$content)
vader_sent2 <- vader_sent %>%
rowid_to_column('id') %>%
mutate(sentiment_vader = case_when(
compound > 0.05 ~ 'positive',
compound < -0.05 ~ 'negative',
.default = 'neutral'
)) %>%
select(id, sentiment_vader) %>%
mutate(
sentiment_vader = as.factor(sentiment_vader))
comparison_df <- comparison_df %>%
left_join(vader_sent2, by = 'id')
accuracy(comparison_df,
sentiment_star,
sentiment_vader)
conf_mat(comparison_df, # VADER has a harder time classifying neutral
sentiment_star,
sentiment_vader)
###---Output---###
accuracy(comparison_df,
sentiment_star,
sentiment_vader)
###---Libraries---###
library(tidyverse)
library(tibble)
library(stringr)
library(tidytext)
library(textstem)
library(tokenizers)
library(reshape2)
library(wordcloud)
###---Input---###
df <- read_csv('data/reviews.csv')
###---Processing---###
# Add manual column with positive, negative, neutral rating based on stars
df <- df %>%
rowid_to_column('id') %>%
mutate(
sentiment_star = case_when(
score < 3 ~ 'negative',
score == 3 ~ 'neutral',
score > 3 ~ 'positive'
))
# Add columns with number of words/characters
df <- df %>%
mutate(n_words = count_words(content),
n_char = nchar(content))
# Plot number of words on histogram to inspect data
df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
theme_minimal() +
scale_x_continuous()
df %>%
summarise(
mean_words = mean(n_words),
mean_char = mean(n_char)
)
# Turn reviews into tokens
tokens <- df %>%
unnest_tokens(word, content) # word is the level, review is column
# Find most common words
common_words <- tokens %>% # Create dataset of most common words
group_by(word) %>%
count(sort = TRUE)
# Remove standard stopwords
tokens_no_stop <- tokens %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[[:digit:]]+"))
# Check most common words again for manual removal
common_words_2 <- tokens_no_stop %>%
group_by(word) %>%
count(sort = TRUE)
# Create custom stopwords
custom_stop_words <-  # words that have no real value, all reviews are about doctors
tibble(
word = c(
'app'
),
lexicon = 'docs' # Create lexicon !!!!
)
# Remove custom stopwords
tokens_no_stop <- tokens_no_stop %>%
anti_join(custom_stop_words)
# Plot Common Words after stopword removal
tokens_no_stop %>%
group_by(word) %>%
count(sort = TRUE) %>%
ungroup() %>%
top_n(25) %>%
ggplot(aes(x = n,
y = reorder(word, n)
)
) +
geom_col() +
scale_y_reordered() +
labs(y = NULL)
# Stemming
tokens_stemmed <- tokens_no_stop %>%
mutate(word_stem = stem_words(word),
word_lemma = lemmatize_words(word))
###---Output---###
write_csv(tokens_stemmed, 'data/tokenized_reviews.csv')
write_csv(df, 'data/cleaned_reviews.csv')
###---Libraries---###
library(tidyverse)
library(tibble)
library(stringr)
library(tidytext)
library(vader)
library(tokenizers)
library(textdata)
library(textstem)
library(yardstick)
###---Input---###
tokens <- read_csv('data/tokenized_reviews.csv')
reviews <- read_csv('data/cleaned_reviews.csv')
###---Process---###
# Sentiment Bing
tokens_bing <- tokens %>%
left_join(get_sentiments('bing')) %>%
mutate(replace_na(sentiment, 'neutral'))
sentiment_bing <- tokens_bing %>%
group_by(id) %>%
count(sentiment) %>%
ungroup() %>%
pivot_wider( # you will normally Google this
names_from = sentiment,
values_from = n,
values_fill = 0
) %>%
mutate(
sentiment_bing = case_when(
positive > negative ~ "positive",
negative > positive ~ "negative",
.default = "neutral"
)
)
# Compare with sentiment star
comparison_df <- reviews %>%
select(id, sentiment_star) %>%
left_join(sentiment_bing, by = "id") %>%
select(id, sentiment_star, sentiment_bing) %>%
mutate(
sentiment_star = as.factor(sentiment_star),
sentiment_bing = as.factor(sentiment_bing)
)
accuracy(comparison_df,
sentiment_star,
sentiment_bing)
conf_mat(comparison_df,
sentiment_star,
sentiment_bing)
# Sentiment Afinn
tokens_afinn <- tokens %>%
left_join(get_sentiments('afinn')) %>%
mutate(value = replace_na(value, 0))
sentiment_afinn <- tokens_afinn %>%
group_by(id) %>%
summarise(score = sum(value, na.rm = TRUE)) %>%
ungroup %>%
mutate(sentiment_afinn = case_when(
score > 0 ~ 'positive',
score < 0 ~ 'negative',
.default = 'neutral'
)) %>%
select(!score)
comparison_df <- comparison_df %>%
left_join(sentiment_afinn, by = 'id')
comparison_df <- comparison_df %>%
mutate(sentiment_afinn = as.factor(sentiment_afinn))
accuracy(comparison_df,
sentiment_star,
sentiment_afinn)
conf_mat(comparison_df,
sentiment_star,
sentiment_afinn)
# compare two methods to themselves
accuracy(comparison_df,
sentiment_bing,
sentiment_afinn)
conf_mat(comparison_df,
sentiment_bing,
sentiment_afinn)
# Sentiment VADER
vader_sent <- vader_df(reviews$content)
vader_sent2 <- vader_sent %>%
rowid_to_column('id') %>%
mutate(sentiment_vader = case_when(
compound > 0.05 ~ 'positive',
compound < -0.05 ~ 'negative',
.default = 'neutral'
)) %>%
select(id, sentiment_vader) %>%
mutate(
sentiment_vader = as.factor(sentiment_vader))
comparison_df <- comparison_df %>%
left_join(vader_sent2, by = 'id')
accuracy(comparison_df,
sentiment_star,
sentiment_vader)
conf_mat(comparison_df, # VADER has a harder time classifying neutral
sentiment_star,
sentiment_vader)
View(vader_sent)
View(vader_sent2)
View(tokens_bing)
sentiment_data <- vader_sent2 %>%
left_join(reviews, by ='id')
View(sentiment_data)
###---Output---###
write_csv(sentiment_data, 'data/sentiment_data.csv')
sentiment_tokens <- vader_sent2 %>%
left_join(reviews, by ='id')
write_csv(sentiment_tokens, 'data/sentiment_tokens.csv')
###---Libraries---###
library(stm)
library(tidyverse)
library(tibble)
library(tidytext)
###---Input---###
tokens <- read_csv('data/sentiment_tokens.csv')
View(tokens)
###---Input---###
tokens <- read_csv('data/tokenized_reviews.csv')
sentiment_tokens <- vader_sent2 %>%
left_join(tokens, by ='id')
write_csv(sentiment_tokens, 'data/sentiment_tokens.csv')
###---Input---###
tokens <- read_csv('data/sentiment_tokens.csv')
View(tokens)
# Establish vocab list to reduce noise
word_counts <- tokens %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 4)
View(word_counts)
###---Input---###
tokens <- read_csv('data/sentiment_tokens.csv') %>%
drop_na(word_lemma)
# Establish vocab list to reduce noise
word_counts <- tokens %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 4)
View(word_counts)
tokens %>%
group_by(id, brand) %>%
count()
tokens %>%
group_by(id, brand) %>%
summarise(n = n())
tokens %>%
group_by(id) %>%
group_by(brand) %>%
summarise(n = n())
tokens %>%
group_by(review_id) %>%
group_by(brand) %>%
summarise(n = n())
tokens %>%
group_by(brand, review_id) %>%
summarise(n = n())
tokens %>%
group_by(review_id) %>%
group_by(brand, .add =TRUE) %>%
summarise(n = n())
tokens %>%
distinct(id, brand) %>%
count(brand)
word_counts_total <- tokens %>%
group_by(word_lemma) %>%
count(sort = TRUE)
tokens <- tokens %>%
filter(word_lemma %in% word_counts$word_lemma)
# Create doc-term matrix
doc_word_count <- tokens %>% # exam
count(id, word_lemma) %>%
ungroup()
View(common_words)
cardio_dtm <- doc_word_count %>% # exam
cast_sparse(id, word_lemma, n)
# Model
cardio_topics <- stm( #exam
cardio_dtm,
K = 10,
seed = 1234567890
)
# Explore output
labelTopics(cardio_topics)
# Split between brands
adidas <- tokens %>%
filter(brand == 'adidas')
nike <- tokens %>%
filter(brand == 'nike')
puma <- tokens %>%
filter(brand == 'puma')
gymshark <- tokens %>%
filter(brand == 'gymshark')
# Establish vocab list to reduce noise
word_counts <- adidas %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 4)
# Establish vocab list to reduce noise
adidas_word_count <- adidas %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 4)
adidas <- adidas %>%
filter(word_lemma %in% adidas_word_count$word_lemma)
# Create doc-term matrix
doc_word_count <- adidas %>%
count(id, word_lemma) %>%
ungroup()
# Create doc-term matrix
adidas_doc_word_count <- adidas %>%
count(id, word_lemma) %>%
ungroup()
cardio_dtm <- adidas_doc_word_count %>%
cast_sparse(id, word_lemma, n)
adidas_cardio_dtm <- adidas_doc_word_count %>%
cast_sparse(id, word_lemma, n)
# Model
adidas_cardio_topics <- stm(
adidas_cardio_dtm,
K = 10,
seed = 1234567890
)
# Explore output
labelTopics(adidas_cardio_topics)
# Establish vocab list to reduce noise
nike_word_count <- nike %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 4)
nike <- nike %>%
filter(word_lemma %in% nike_word_count$word_lemma)
# Create doc-term matrix
nike_doc_word_count <- nike %>%
count(id, word_lemma) %>%
ungroup()
nike_cardio_dtm <- nike_doc_word_count %>%
cast_sparse(id, word_lemma, n)
# Model
nike_cardio_topics <- stm(
nike_cardio_dtm,
K = 10,
seed = 1234567890
)
# Explore output
labelTopics(nike_cardio_topics)
###---PUMA---###
# Establish vocab list to reduce noise
puma_word_count <- puma %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 4)
puma <- puma %>%
filter(word_lemma %in% puma_word_count$word_lemma)
# Create doc-term matrix
puma_doc_word_count <- puma %>%
count(id, word_lemma) %>%
ungroup()
puma_cardio_dtm <- puma_doc_word_count %>%
cast_sparse(id, word_lemma, n)
# Model
puma_cardio_topics <- stm(
puma_cardio_dtm,
K = 10,
seed = 1234567890
)
# Explore output
labelTopics(puma_cardio_topics)
# Split between brands AND positive/negative reviews
adidas_pos <- tokens %>%
filter(brand == 'adidas' | sentiment_vader == 'positive')
adidas_neg <- tokens %>%
filter(brand == 'adidas' | sentiment_vader == 'negative')
# Split between brands AND positive/negative reviews
adidas_pos <- tokens %>%
filter(brand == 'adidas' AND sentiment_vader == 'positive')
# Split between brands AND positive/negative reviews
adidas_pos <- tokens %>%
filter(brand == 'adidas' & sentiment_vader == 'positive')
adidas_neg <- tokens %>%
filter(brand == 'adidas' & sentiment_vader == 'negative')
# Split between brands AND positive/negative reviews
adidas_pos <- tokens %>%
filter(brand == "adidas" & sentiment_vader == "positive")
adidas_neg <- tokens %>%
filter(brand == "adidas" & sentiment_vader == "negative")
nike_pos <- tokens %>%
filter(brand == "nike" & sentiment_vader == "positive")
nike_neg <- tokens %>%
filter(brand == "nike" & sentiment_vader == "negative")
gymshark_pos <- tokens %>%
filter(brand == "gymshark" & sentiment_vader == "positive")
gymshark_neg <- tokens %>%
filter(brand == "gymshark" & sentiment_vader == "negative")
puma_pos <- tokens %>%
filter(brand == 'puma' & sentiment_vader == "positive")
puma_neg <- tokens %>%
filetr(brand == 'puma' & sentiment_vader == 'negative')
puma_neg <- tokens %>%
filter(brand == 'puma' & sentiment_vader == 'negative')
