###---Libraries---###
library(tidyverse)
library(tibble)
library(stringr)
library(tidytext)
library(textstem)
library(tokenizers)
library(reshape2)
library(wordcloud)
###---Input---###
data_raw <- read_csv('data/reviews.csv')
rm(data_raw)
###---Input---###
df <- read_csv('data/reviews.csv')
View(df)
source("C:/Users/krisb/OneDrive/Bureaublad/Programming_projects/sportswear-app-reviews-sentiment-analysis/preprocess.R", echo = TRUE)
###---Processing---###
df <- df %>%
mutate(n_words = count_words(content),
n_char = nchar(content))
df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
facet_wrap(~sentiment_star_binary) +
theme_bw() +
scale_x_continuous()
df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
theme_bw() +
scale_x_continuous()
df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
theme_minimal() +
scale_x_continuous()
df %>%
summarise(
mean_words = mean(n_words),
mean_char = mean(n_char)
)
tokens <- df %>%
unnest_tokens(word, content) # word is the level, review is column
common_words <- tokens %>% # Create dataset of most common words
group_by(word) %>%
count(sort = TRUE)
View(common_words)
stop_words
# Remove standard stopwords
tokens_no_stop <- tokens %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[[:digit:]]+"))
# Check most common words again for manual removal
common_words_2 <- tokens_no_stop %>%
group_by(word) %>%
count(sort = TRUE)
View(common_words_2)
# Create custom stopwords
custom_stop_words <-  # words that have no real value, all reviews are about doctors
tibble(
word = c(
'app'
),
lexicon = 'docs' # Create lexicon !!!!
)
# Create custom stopwords
custom_stop_words <-  # words that have no real value, all reviews are about doctors
tibble(
word = c(
'app'
),
lexicon = 'docs' # Create lexicon !!!!
)
# Remove custom stopwords
tokens_no_stop <- tokens_no_stop %>%
anti_join(custom_stop_words)
# Plot Common Words after stopword removal
tokens_no_stop %>%
group_by(word) %>%
count(sort = TRUE) %>%
ungroup() %>%
top_n(25) %>%
ggplot(aes(x = n,
y = reorder(word, n)
)
) +
geom_col() +
scale_y_reordered() +
labs(y = NULL)
# Stemming
tokens_stemmed <- tokens_no_stop %>%
mutate(word_stem = stem_words(word),
word_lemma = lemmatize_words(word))
View(tokens_stemmed)
ar != 'neutral') %>%
# Create wordcloud
tokens_stemmed %>%
filter(sentiment_star != 'neutral') %>%
count(word_lemma, sentiment_star, sort = TRUE) %>%
acast(word_lemma ~ sentiment_star, value.var = "n", fill = 0) %>%
commonality.cloud(#colors = c("red", "blue"),
max.words = 50)
###---Output---###
write_csv(tokens_stemmed, 'data/tokenized_reviews.csv')
